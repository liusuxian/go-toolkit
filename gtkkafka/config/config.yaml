# kafka 配置
kafka:
  servers: "host.docker.internal:9092" # SSL接入点的IP地址以及端口
  protocol: "PLAINTEXT" # SASL用户认证协议
  retryDelay: "10s" # 当消费失败时重试的间隔时间，默认 10s
  retryMaxCount: 0 # 当消费失败时重试的最大次数，默认 0无限重试
  batchSize: 200 # 批量消费的条数，默认 200
  batchInterval: "5s" # 批量消费的间隔时间，默认 5s
  isClose: true # 是否不启动 Kafka 客户端（适用于本地调试有时候没有kafka环境的情况）
  env: "test" # 当前服务环境，默认 local
  producerConsumerConfigMap: # 生产者和消费者配置
    test_100:
      topic: "topic_100" # topic 名称
      startProducer: false # 启动生产者
      startConsumer: false # 启动消费者
      startAll: true # 启动生产者和消费者
    test_101:
      topic: "topic_101" # topic 名称
      startProducer: false # 启动生产者
      startConsumer: false # 启动消费者
      startAll: true # 启动生产者和消费者
    test_102:
      topic: "topic_102" # topic 名称
      startProducer: false # 启动生产者
      startConsumer: false # 启动消费者
      startAll: true # 启动生产者和消费者
  excludeEnvTopicMap: # 指定哪些服务环境下对应的哪些 Topic 不发送 Kafka 消息
    test: ["topic_101", "topic_102"]
  logConfig: # 日志配置
    logPath: "logs/kafka" # 日志文件路径，默认 logs
    logType: "text" # 日志类型，json|text，默认 text
    logLevel: "debug" # 日志级别，panic、fatal、error、warning、info、debug、trace，默认 debug
    ctxKeys: [] # 自定义 Context 上下文变量名称，自动打印 Context 的变量到日志中，默认为空
    logLevelFileName: # 日志级别所对应的日志文件名称，默认 gtklog.log
      panic: "error.log"
      fatal: "error.log"
      error: "error.log"
      warning: "access.log"
      info: "access.log"
      debug: "access.log"
      trace: "access.log"
    fileNameDateFormat: "%Y-%m-%d" # 文件名的日期格式，默认 %Y-%m-%d
    timestampFormat: "2006-01-02 15:04:05.000" # 日志中日期时间格式，默认 2006-01-02 15:04:05.000
    fileInfoField: "caller" # 文件名和行号字段名，默认 caller
    jsonPrettyPrint: true # json日志是否美化输出，默认 false
    jsonDataKey: "" # json日志条目中，数据字段都会作为该字段的嵌入字段，默认为空
    maxAge: "168h" # 保留旧日志文件的最长时间，默认 7天
    rotationTime: "24h" # 日志轮转的时间间隔，默认 24小时
    rotationSize: 5368709120 # 日志文件达到指定大小时进行轮转，默认 1024*1024*1024*5
    stdout: true # 是否输出到控制台，默认 false
